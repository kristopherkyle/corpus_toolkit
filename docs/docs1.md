
# corpus-toolkit Documentation Page
This page includes details on the arguments each function in the corpus-toolkit package takes.

**Note that this page is in progress!** All (heavily commented) code is [also available here](https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py)

## Default lists

```python
default_punct_list = [",",".","?","'",'"',"!",":",";","(",")","[","]","''","``","--"] #we can add more items to this if needed
default_space_list = ["\n","\t","    ","   ","  "]
ignore_list = [""," ", "  ", "   ", "    "] #list of items we want to ignore in our frequency calculations
```

## ldcorpus()

This function will load all files that match a certain filename ending (e.g., ".txt") in a folder. By default it loads all files ending in ".txt" and prints the name of each file being loaded.

**ldcorpus()** Is a generator function that loads all corpus files in a folder. It takes three arguments (two of which have default values):
- **dirname** (*string variable*) This is the name of the directory that one's files are in. It will not gather files in nested folders.
- **ending** (*string variable*) This is the ending for your target filenames. By default, this is ".txt".
- **verbose** (*Boolean variable*) This determines whether filenames are printed to the console when loading. By default, this is set to "True"

```python
def ldcorpus(dirname,ending = ".txt",verbose = True):
		filenames = glob.glob(dirname + "/*" + ending) #gather all text names
		nfiles = len(filenames) #get total number of files in corpus
		fcount = 0 #counter for corpus files
		for x in filenames:
			fcount +=1 #update file count
			sm_fname = x.split(dirsep)[-1] # get filename
			if verbose == True:
				print("Processing", sm_fname, "(" + str(fcount), "of", nfiles,"files)")
			text = open(x, errors = "ignore").read()
			yield(text)
```
## tokenize()

**tokenize()** Is a generator function that tokenizes a list of texts. It takes eight arguments (seven of which have default values):
- **corpus** (*list of texts*) This is a list of corpus texts (strings)
- **remove_list** (*list of characters*) This is a list of characters to be removed from each text. By default this is the `default_punct_list`
- **space_list** (*list of characters*) This is a list of characters (and character sequences) to be replaced by a single space. By default this is the `default_space_list`
- **split_token** (*string variable*) This is the character used to split the text string. By default this is a single space `" "`.
- **lower** (*Boolean variable*) This is a Boolean value that determines whether all characters in each text are set to lower case. By default, this is true.
- **lemma** (*dictionary*) This is the lemma dictionary used to lemmatize tokens in each text that consists of lower-case unlemmatized words as keys and lemmas as values. By default, this is a pre-loaded lemma list. If set to False, then texts are not lemmatized.
- **ngram** (*Boolean variable or integer*) This sets the n-gram length for tokenization. By default, this is set to False.
- **ngrm-connect** (*string variable*) This sets the character used to join words in an ngram. By default, this is set to `"__"`

```python
def tokenize(corpus, remove_list = default_punct_list, space_list = default_space_list, split_token = " ", lower = True, lemma=lemma_dict,ngram = False,ngrm_connect = "__"):
	for text in corpus: #iterate through each string in the corpus_list
		for item in remove_list:
			text = text.replace(item,"") #replace each item in list with "" (i.e., nothing)
		for item in space_list:
			text = text.replace(item," ")
		if lower == True:
			text = text.lower()
		#then we will tokenize the document
		tokenized = text.split(split_token) #split string into list using the split token (by default this is a space " ")
		if lemma != False: #if lemma isn't False
			tokenized = lemmatize(tokenized,lemma)
		if ngram != False:
			tokenized = ngrammer(tokenized,ngram,ngrm_connect)

		yield(tokenized)
```

## frequency()

The **frequency()** function takes a list of tokenized corpus texts (i.e., a list of lists) such as that generated by the **tokenize()** function and returns a frequency (or range) dictionary with linguistic items (e.g., words) as keys and frequency (or range) values as values. The **frequency()** function takes four arguments (three of which have default values):
- **corpus_list** (*list of lists*) This is a list consisting of tokenized texts (represented as lists of strings).
- **ignore** (*list of strings*) This is a list of strings to ignore when calculating frequency. By default, this is the pre-defined `ignore_list`.
- **calc** (*string*) This indicates whether the function will produce frequency or range values. The options are `freq` (default) or `range`.
- **normed** (*Boolean value*) This indicates whether frequencies are normed (per million words) or represent raw frequencies. By default, this is set to `False` (raw frequencies are reported).

```python
def frequency(corpus_list, ignore = ignore_list, calc = 'freq', normed = False): #options for calc are 'freq' or 'range'
	freq_dict = {} #empty dictionary

	for tokenized in corpus_list: #iterate through the tokenized texts
		if calc == 'range': #if range was selected:
			tokenized = list(set(tokenized)) #this creates a list of types (unique words)

		for token in tokenized: #iterate through each word in the texts
			if token in ignore_list: #if token is in ignore list
				continue #move on to next word
			if token not in freq_dict: #if the token isn't already in the dictionary:
				freq_dict[token] = 1 #set the token as the key and the value as 1
			else: #if it is in the dictionary
				freq_dict[token] += 1 #add one to the count

	### Normalization:
	if normed == True and calc == 'freq':
		corp_size = sum(freq_dict.values()) #this sums all of the values in the dictionary
		for x in freq_dict:
			freq_dict[x] = freq_dict[x]/corp_size * 1000000 #norm per million words
	elif normed == True and calc == "range":
		corp_size = len(corpus_list) #number of documents in corpus
		for x in freq_dict:
			freq_dict[x] = freq_dict[x]/corp_size * 100 #create percentage (norm by 100)

	return(freq_dict)
```

## head()

The **head()** function takes a dictionary of word (or n-gram) keys with a statistic (e.g., frequency) as the values and returns a sorted representation of the dictionary. The main purpose of the **head()** function is to quickly check any lists generated by other functions. It can print the results, save the sorted list as a Python object, or write a sorted list to file. The **head()** function takes six arguments (five of which have default values):
- **stat_dict** (*dictionary*) This is a dictionary of item : statistic key - value pairs (e.g., the output of the **frequency()** function).
- **hits** (*integer*) This is an integer indicating how many items in the sample should be printed. (Note that this is ignored if writing to a file or saving a Python object). By default, this is set to `20`.
- **hsort** (*Boolean value*) If `True` the values are sorted from largest to smallest. If `False`, the values are sorted from smallest to largest. By default, this is set to `True`.
- **output** (*Boolean value*) If `True`, the **head()** function outputs a list consisting of (item, statistic) tuples. If set to `True`, no items are printed to the console. By default, this is set to `False`.
- **filename** (*string*) Providing a filename (e.g., "my_frequency.txt") string will cause the **head()** function to write a spreadsheet file including all items and their statistic (e.g., frequency value). By default, this is set to `None`, and no file is written.
- **sep** (*string*) This sets the character(s) used to separate columns when lists are written to a file. By default, this is a tab character `"\t"`

```python
def head(stat_dict,hits = 20,hsort = True,output = False,filename = None, sep = "\t"):
	#first, create sorted list. Presumes that operator has been imported
	sorted_list = sorted(stat_dict.items(),key=operator.itemgetter(1),reverse = hsort)[:hits]

	if output == False and filename == None: #if we aren't writing a file or returning a list
		for x in sorted_list: #iterate through the output
			print(x[0] + "\t" + str(x[1])) #print the sorted list in a nice format

	elif filename is not None: #if a filename was provided
		outf = open(filename,"w") #create a blank file in the working directory using the filename
		for x in sorted_list: #iterate through list
			outf.write(x[0] + sep + str(x[1])+"\n") #write each line to a file using the separator
		outf.flush() #flush the file buffer
		outf.close() #close the file

	if output == True: #if output is true
		return(sorted_list) #return the sorted list

```
